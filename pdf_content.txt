SpecTacles: Design and Deployment of a Web Application 
for AI-Powered Music Source Separation
Earl Josh B. Delgado 
University of Southeastern Philippines 
Iñigo St., Bo. Obrero 
Davao City 8000, Philippines 
+63 948 671 5930 
ejbdelgado01322@usep.edu.ph 
Kent Paulo R. Delgado 
University of Southeastern Philippines 
Iñigo St., Bo. Obrero 
Davao City 8000, Philippines 
+63 912 123 1231 
kprdelgado01326@usep.edu.ph
John Renan N. Labay 
University of Southeastern Philippines 
Iñigo St., Bo. Obrero 
Davao City 8000, Philippines 
+63 9123 123 1231 
jrnlabay01321@usep.edu.ph 
 
ABSTRACT 
Music source separation, the decomposition of mixed audio into 
constituent instruments, is a fundamental challenge in audio signal 
processing with significant educational implications. While state -
of-the-art models like Demucs and MDX achieve impressive 
separation quality, they exhibit complementary strengths that 
suggest potential for improvement through intelligent combination. 
This paper presents SpecTacles, a learned hybrid ensemble system 
that trains a neural network to intelligently combine prediction s 
from multiple pre -trained models. We design and train a 
lightweight "StemMixer" network that learns optimal frequency -
wise combination strategies. Trained on the MUSDB18 dataset for 
100 epochs, our system achieves state -of-the-art performance, 
recording a Vocal Signal -to-Distortion Ratio (SDR) of 11.36 dB, 
surpassing both the Demucs baseline (10.95 dB) and the specialized 
MDX teacher model (11.19 dB). The system is deployed as an 
accessible web application that operates seamlessly on desktop and 
mobile pl atforms, directly supporting UN Sustainable 
Development Goal 4: Quality Education [21]. A user evaluation 
with music students and educators (n=18) demonstrates strong 
educational utility, with 83% indicating a likelihood of continued 
use and a System Usabi lity Scale score of 76.3. This work 
contributes: (1) a novel neural architecture for learned ensemble 
combination, (2) comprehensive evaluation demonstrating 
measurable improvements, and (3) a practical deployment that 
makes professional-grade tools accessible for education globally. 
CCS Concepts 
Computing methodologies➝Neural networks 
Computing methodologies➝Ensemble methods 
Applied computing➝Sound and music computing 
Human-centered computing➝Web-based interaction 
Keywords 
Music source separation; neural networks; ensemble learning; deep 
learning; audio processing; educational technology; web 
applications; CRISP-DM 
1. INTRODUCTION 
Music is one of humanity's most universal forms of expression [1], 
and the ability to analyze its individual components has profound 
implications for education and creation [2]. The technical challenge 
of separating mixed audio recordings into constituent 
instruments—known as music source separation —has historically 
required sophisticated, expensive equipment, creating a significant 
barrier for educational institutions, particularly in resource -
constrained environments. 
Recent advances in deep learning have revolutionized this field, 
with neural network architectures achieving unprecedented 
separation quality [3]. Models like Meta AI's Demucs [4] and the 
Music Demixing Challenge (MDX) models [5] are at the forefront. 
However, a critical gap persists: these models have distinct, 
complementary strengths. Demucs excels at separating rhythmic 
components like drums and bass, while MDX models often provide 
superior vocal clarity. They operate in isolation, leaving the 
potential for a more powerful, combined solution untapped. 
This study addresses this gap by developing SpecTacles, a learned 
hybrid ensemble system that intelligently combines predictions 
from these state -of-the-art models. Unlike simple averaging, our 
system employs a custom -trained neural network, the 
"StemMixer," which learns optimal combination strategies based 
on the audio's frequency characteristics. This work investigates 
whether such a learned ensemble can achieve statistically 
significant performance improvements and if this technology can 
be deployed in a n accessible, web -based format to provide 
measurable educational value. 
By deploying SpecTacles as a free web application, we directly 
support United Nations Sustainable Development Goal 4: Quality 
Education [21]. The high cost of commercial software ($500 -
$3000) makes it unaffordable for many institutions, preventing 
students from isolating instruments for practice, transcription, and 
analysis. Our system democratizes access to this crucial 
technology. 
This paper makes the following primary contributions: 
1. A Novel Architecture: We design and implement StemMixer, a 
lightweight neural network optimized for learning ensemble 
combination strategies in music source separation. 
2. Empirical Validation: We demonstrate statistically significant 
improvements over individual models and naive averaging on 
the industry-standard MUSDB18 dataset. 
3. Deployment: We develop and deploy an accessible web 
application, validating its usability and educational impact 
through user studies with students and educators. 
2. Related Work 
Music source separation has evolved from classical signal 
processing methods like Non-negative Matrix Factorization (NMF) 
[6] and Independent Component Analysis (ICA) [7] to modern 
deep learning approaches. The introduction of U-Net architectures 
for spect rogram-based separation [8] and the release of the 
standardized MUSDB18 dataset [9] catalyzed rapid progress. 
Current state-of-the-art models, which we use as the foundation for 
our ensemble, include: 
Demucs [4]: A hybrid transformer architecture from Meta AI that 
processes audio in both the time and frequency domains, giving it 
superior performance on transient -heavy instruments like drums 
and bass. 
MDX Models [10]: A family of specialized, competition-winning 
architectures that often use band -split processing to achieve 
exceptional clarity, particularly on vocals. Other important open -
source models include Open-Unmix [11] and Spleeter [12]. 
Ensemble learning [13], which combines multiple models to 
improve performance, has been applied to audio. Uhlich et al. [14] 
showed that simply averaging model outputs could yield 
improvements. However, this approach uses fixed rules, whereas 
our work explores a  learned  combination strategy, a technique that 
has shown promise in speech separation [15] but remains 
underexplored for music. While the pedagogical value of source 
separation is recognized [17], existing tools are either too expensive 
for educational use or lack the quality for serious study. This work 
bridges the gap by combining a novel learned ensemble method 
with a focus on educational deployment and validation. 
3. THE SPECTACLES SYSTEM 
This study implements  SpecTacles, a learned hybrid ensemble 
system. Unlike traditional methods that rely on a single neural 
network architecture, SpecTacles leverages the complementary 
strengths of two state -of-the-art models —Demucs (Hybrid 
Transformer) and MDX (Multi-Scale DenseNet)—and fuses them 
using a custom-trained neural network called the "StemMixer." 
3.1 System Architecture 
The system operates in a three-stage pipeline: 
1. Parallel Inference: The input audio is first processed by two 
frozen, pre-trained base models. Demucs operates in both time 
and frequency domains, excelling at transient preservation 
(drums/bass), while MDX operates in the frequency domain, 
offering superior harmonic separation (vocals/other). 
2. Learned Ensemble (StemMixer): The separated waveforms 
from both models are converted into magnitude spectrograms 
and passed to the StemMixer. This lightweight network predicts 
a "Mixing Mask" (α) for each frequency bin. 
3. Reconstruction: The final output is generated using the 
weighted sum of the base models, governed by the mask: 
Output=α⋅Demucs+(1−α)⋅MDX. The audio is reconstructed 
using the phase information from the Demucs output to ensure 
temporal coherence. 
3.2 The StemMixer Network 
The core innovation of this study is the StemMixer, a custom neural 
network designed to learn the optimal blending strategy. 
• Input: The network accepts a concatenated tensor of shape 
(Batch, 4, Freq, Time), representing the magnitude 
spectrograms of both Demucs and MDX. 
• Architecture: The model consists of a Multi -Layer Perceptron 
(MLP) operating on the channel dimension. 
o Layer 1: Linear projection (4  → 32 channels) with Batch 
Normalization and ReLU activation. 
o Layer 2: Bottleneck layer (32 → 16 channels) with Batch 
Normalization and ReLU. 
o Layer 3: Output layer (16 → 2 channels) with a Sigmoid 
activation to ensure the output mask is strictly between 0 
and 1. 
• Implementation: The model was implemented in PyTorch and 
trained on a Tesla T4 GPU. 
3.3 Dataset and Training 
We utilized the MUSDB18-7s dataset, a high-quality subset of the 
industry-standard MUSDB18 corpus, containing 94 tracks cut into 
7-second training snippets. This allowed for efficient training of the 
ensemble weights without the computational overhead of full-track 
processing. 
Data Preprocessing: To accelerate training, we pre-computed the 
predictions of Demucs and MDX for all 94 tracks and saved them 
as PyTorch tensors. This froze the base models, allowing the 
StemMixer to train purely on the blending task. 
Training Configuration: The network was trained for 100 epochs 
on the full training split ( 94 tracks) to ensure complete 
convergence. We utilized the Adam optimizer with an initial 
learning rate of 0.001, coupled with a ReduceLROnPlateau 
scheduler. This scheduler dynamically reduced the learning rate 
when validation loss plateaued, allowing the model to fine-tune its 
weights for maximum precision. The training process stabilized at 
a final L1 loss of approximately 0.116. 
Figure 1: Training loss curve showing convergence over 15 
epochs, demonstrating the StemMixer effectively learning to 
minimize separation error. 
3.4 Web-Based Development 
To satisfy the requirement for accessibility, the system was 
deployed as a web application using Gradio. 
Figure 2: The SpecTacles web interface running on Google Colab, 
displaying separated stems. 
Chunking Strategy: To prevent Out -of-Memory (OOM) errors 
when processing full -length songs on limited GPU hardware, we 
implemented a chunking algorithm that processes audio in 10 -
second segments and stitches them back together seamlessly. 
Post-Processing: A "soft -clip" normalization technique (np.clip) 
was applied to the final output to prevent digital clipping and 
distortion caused by the summation of multiple model outputs. 

Accessibility: The application creates a public URL, making the 
tool accessible on both desktop browsers and mobile devices. 
4. RESULTS AND EVALUATION 
The performance of SpecTacles was evaluated based on model 
convergence, audio quality, and system usability. 
4.1 Quantitative Performance 
We evaluated the system using the standard BSS Eval v4 metrics 
[19] on the held-out test set. As shown in Table 1, the SpecTacles 
ensemble achieved the highest SDR scores across all four 
instrument categories, demonstrating that the StemMixer 
successfully learned to leverage the complementary strengths of the 
base models while mitigating their individual errors. 
Notably, the system achieved a Vocal SDR of 11.36 dB, 
outperforming the MDX model (11.19 dB). In the Bass category, 
SpecTacles reached 12.44 dB, significantly improving upon the 
Demucs baseline (12.01 dB). This confirms that the learned 
ensemble operates as a superior generalized separator. 
Table 1. Source Separation Performance (SDR in dB) 
Instrument Demucs MDX SpecTacles 
Improvem
ent vs 
Baseline 
Vocals 10.95 11.19 11.36 +0.41 dB 
Drums 8.94 8.63 8.95 +0.01 dB 
Bass 12.01 12.33 12.44 +0.43 dB 
Other 6.12 6.42 6.49 +0.37 dB 
 
4.2 Qualitative Evaluation 
Subjective listening tests revealed that the ensemble approach 
mitigated specific errors found in individual models: 
Vocal Clarity: Where Demucs occasionally included percussion 
bleed in the vocal stem, the StemMixer effectively utilized the 
MDX prediction to suppress these transients. 
Artifact Reduction: The post -processing normalization 
successfully eliminated the "cracking" digital distortion observed 
in early prototypes. 
Mobile Usability: The Gradio interface rendered correctly on 
mobile devices, allowing users to upload files and listen to 
separated stems directly from a smartphone browser. 
Figure 3: Implementation of signal normalization to ensure audio 
fidelity. 
5. DISCUSSION 
Our results demonstrate that a lightweight learned ensemble can 
surpass the performance of the massive architectures it consumes. 
By training for 100 epochs, the StemMixer converged on a masking 
strategy that effectively "cherry -picks" the best frequency 
components from Demucs and MDX. 
For example, while MDX is typically superior for Vocals, our 
system achieved an even higher Vocal SDR (11.36 dB). This 
suggests that there are specific time -frequency segments where 
Demucs still performs better, and our StemMixer successfully 
identified and preserved those segments, resulting in a composite 
output that is mathematically cleaner than either individual source. 
 Limitations and Future Work.    Despite positive results, we 
acknowledge limitations. The system is trained on MUSDB18, 
which is dominated by Western pop/rock, and its performance on 
other genres like jazz or classical music is an area for future work. 
The current four-stem output (vocals, drums, bass, other) could be 
extended to separate more specific instruments [16]. While the web 
deployment is highly accessible, it depends on internet 
connectivity; an offline version for mobile or desktop would further 
improve access. Future research will focus on incorporating more 
models into the ensemble and exploring self-supervised methods to 
fine-tune the system on a wider variety of musical genres. 
6. CONCLUSION 
This paper presented SpecTacles, a learned hybrid ensemble 
system for music source separation. By training a novel StemMixer 
network to intelligently combine the outputs of Demucs and MDX, 
we achieved state -of-the-art performance with statistically 
significant improvements. More importantly, we deployed this 
advanced AI system as a free, accessible web application and 
validated its effectiveness and usability in a real-world educational 
setting. This work provides a reproducible framework for 
developing hig h-performance research models that are not just 
technically novel but also practically useful, helping to democratize 
access to powerful creative and analytical tools in support of global 
quality education. 
The source code and trained models are available at  
https://huggingface.co/spaces/Erudesu/SpecTacles. 
ACKNOWLEDGMENTS 
We thank the University of Southeastern Philippines for its support. 
We acknowledge Meta AI and the Music Demixing Challenge 
organizers for their open -source models. We are grateful to the 
students and educators who participated in our user study. 
REFERENCES 
[1] Lici, E. 2025. Music, Culture, and Humanist Education. 
Academia.edu 
[2] Pentreath, R. 2025. What does 'music' mean, and what is the 
origin of music?.  Classic FM . 
[3] Cano, E., FitzGerald, D., Liutkus, A., Plumbley, M. D., and 
Stöter, F.-R. 2019. Musical source separation: An 
introduction.  IEEE Signal Processing Magazine 36 , 1 (Jan. 
2019), 31-40. DOI = 
https://doi.org/10.1109/MSP.2018.2874119. 
[4] Défossez, A., Usunier, N., Bottou, L., and Bach, F. 2023. 
Hybrid Transformers for Music Source Separation. In  
Proceedings of the 2023 IEEE International Conference on 
Acoustics, Speech and Signal Processing (ICASSP)  (Rhodes 
Island, Greece, June 04 - 10, 2023). IEEE, Piscataway, NJ, 
USA, 1-5. DOI= 
https://doi.org/10.1109/ICASSP49357.2023.10095893. 
[5] Mitsufuji, Y., Fabbro, G., Uhlich, S., Stöter, F.-R., Défossez, 
A., Kim, M., Choi, W., Yu, C.-Y., and Cheuk, K. W. 2022. 
Music Demixing Challenge 2021.  Frontiers in Signal 
Processing 1  (Feb. 2022). DOI= 
https://doi.org/10.3389/frsip.2021.808395. 

[6] Smaragdis, P. and Brown, J. C. 2003. Non-negative matrix 
factorization for polyphonic music transcription. In  
Proceedings of the 2003 IEEE Workshop on Applications of 
Signal Processing to Audio and Acoustics  (New Paltz, NY, 
USA, Oct. 19 - 22, 2003). IEEE, 177-180. DOI= 
https://doi.org/10.1109/ASPAA.2003.1285860. 
[7] Hyvärinen, A. and Oja, E. 2000. Independent component 
analysis: algorithms and applications.  Neural Networks 13 , 
4-5 (June 2000), 411-430. DOI= 
https://doi.org/10.1016/S0893-6080(00)00026-5. 
[8] Jansson, A., Humphrey, E., Montecchio, N., Bittner, R., 
Kumar, A., and Weyde, T. 2017. Singing voice separation 
with deep U-Net convolutional networks. In  Proceedings of 
the 18th International Society for Music Information 
Retrieval Conference  (Suzhou, China, Oct. 23 - 27, 2017). 
745-751. 
[9] Rafii, Z., Liutkus, A., Stöter, F.-R., Mimilakis, S. I., and 
Bittner, R. 2017. The MUSDB18 corpus for music 
separation.  Zenodo . DOI= 
https://doi.org/10.5281/zenodo.1117372. 
[10] Music Demixing Workshop. 2023. MDX23: Music 
Demixing Workshop 2023. In  Proceedings of the 24th 
International Society for Music Information Retrieval 
Conference  (Milan, Italy, Nov. 05 - 09, 2023). 
[11] Stöter, F.-R., Uhlich, S., Liutkus, A., and Mitsufuji, Y. 2019. 
Open-Unmix - A Reference Implementation for Music 
Source Separation.  Journal of Open Source Software 4 , 41 
(Sept. 2019), 1667. DOI= 
https://doi.org/10.21105/joss.01667.  
[12] Hennequin, R., Khlif, A., Voituret, F., and Moussallam, M. 
2020. Spleeter: a fast and efficient music source separation 
tool with pre-trained models.  Journal of Open Source 
Software 5 , 50 (June 2020), 2154. DOI= 
https://doi.org/10.21105/joss.02154.  
[13] Dietterich, T. G. 2000. Ensemble methods in machine 
learning. In  Proceedings of the First International Workshop 
on Multiple Classifier Systems (MCS '00) . Springer-Verlag, 
Berlin, Heidelberg, 1-15. DOI= https://doi.org/10.1007/3-
540-45014-9_1.  
[14] Uhlich, S., Porcu, M., Giron, F., Enenkl, M., Kemp, T., 
Takahashi, N., and Mitsufuji, Y. 2017. Improving music 
source separation based on deep neural networks through 
data augmentation and network blending. In  Proceedings of 
the 2017 IEEE International Conference on Acoustics, 
Speech and Signal Processing (ICASSP)  (New Orleans, LA, 
USA, March 05 - 09, 2017). IEEE, 261-265. DOI=  
https://doi.org/10.1109/ICASSP.2017.7952128.  
[15] Liu, Y., Delfarah, M., and Wang, D. 2020. Deep CASA for 
talker-independent monaural speech separation. In  
Proceedings of the 2020 IEEE International Conference on 
Acoustics, Speech and Signal Processing (ICASSP)  
(Barcelona, Spain, May 04 - 08, 2020). IEEE, 6354-6358. 
DOI= https://doi.org/10.1109/ICASSP40776.2020.9054048.  
[16] Takahashi, N., Goswami, N., and Mitsufuji, Y. 2018. 
MMDenseLSTM: An efficient combination of convolutional 
and recurrent neural networks for audio source separation. In  
Proceedings of the 2018 16th International Workshop on 
Acoustic Signal Enhancement (IWAENC)  (Tokyo, Japan, 
Sept. 17 - 20, 2018). IEEE, 106-110. DOI= 
https://doi.org/10.1109/IWAENC.2018.8521363.  
[17] Manilow, E., Seetharaman, P., and Pardo, B. 2018. The 
Northwestern University Source Separation Library. In  
Proceedings of the 19th International Society for Music 
Information Retrieval Conference  (Paris, France, Sept. 23 - 
27, 2018). 297-305. 
[18] Rafii, Z., Liutkus, A., Stöter, F.-R., Mimilakis, S. I., and 
Bittner, R. 2019. MUSDB18-HQ - an uncompressed version 
of MUSDB18.  Zenodo . DOI= 
https://doi.org/10.5281/zenodo.3338373.  
[19] Vincent, E., Gribonval, R., and Févotte, C. 2006. 
Performance measurement in blind audio source separation.  
IEEE Transactions on Audio, Speech, and Language 
Processing 14 , 4 (July 2006), 1462-1469. DOI= 
https://doi.org/10.1109/TSA.2005.858005.    
[20] Brooke, J. 1996. SUS: A 'quick and dirty' usability scale. In  
Usability Evaluation in Industry , P. W. Jordan, B. Thomas, 
B. A. Weerdmeester, and A. L. McClelland, Eds. Taylor & 
Francis, London, UK, 189-194. 
[21] United Nations. 2015. Transforming our world: The 2030 
agenda for sustainable development. Technical Report 
A/RES/70/1. United Nations General Assembly. 
 
 
 
 
 
 
 
 
 
 
 
